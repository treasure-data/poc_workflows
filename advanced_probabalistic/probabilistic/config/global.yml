# sink_database: cdp_unification_probabilistic_${sub} 
model_config_table: datamodel_build_history
api_server: 'api.treasuredata.com'
create_dashboard: 'no'

base_table:                                       #Used ONLY if you plan to concat some of the original id_cols into a composite key
source_db: ${stg}_probabilistic_${sub}                    #DB where the source input table for probabilistic lives
# input_table: address                          #name of input table
id_col: index                                 #name of canonica_id column created by Deterministic Unification

######## EDA & EXCLUSION OF OVERMERGED IDs ################
run_eda: 'no'                                    #Run EDA framework to get Top-K Strings by value count to esnure there is not over-merging on faulty string values such as '/' or '-' etc.
exclude_overmerging: 'no'                         #yes - autmatically excludes the Top-K strings from the blocking/clustering process of probabilistic
top_k_vals: 100                                    #Top-K string values with the highest value counts you want to see from EDA and exclude from final unifcation


#--- columns used for Unification and their Types
# dedupe_columns: ${probabilistic.dedupe_columns}

# output_suffix: ${probabilistic.output_suffix}                        #gets added at the end of output tables to track different runs on different intput data

# output_table: ${probabilistic.output_table}    #name of final clsuter_table with probabilistic matched IDs
cluster_col_name : cluster_id                     #name of the distinct clsuter_id column assigned to records that match
custom_filter: 'no'                               #if set to 'yes', the final approval logic param for each column will apply

#----  max_records_allowed ----> Number of records allowed per blocking process. If table size exceeds this then it will be split in chunks of that and processed in parallel to avoid memory limits.
#----  record_limit ----> Number of records to be processed by the Python code for the final clsutering/agorithm. If final blocking table exceeds this limit it will be split in chunks and processed in parallel docker images.

# blocking_table: ${probabilistic.blocking_table}    #name of the blocking table
# max_records_allowed: ${probabilistic.max_records_allowed} 
# record_limit: ${probabilistic.record_limit} 


#---- Increasing the value of hashes,keygroups and jaccard_similarity_threshold will increase the Recall
#----                      hashes----> Number of hash functions
#----                   keygroups----> Number of hash values used for creating a signature
#----jaccard_similarity_threshold----> minimum jaccard similarity threshold to create blocks. The higher it is, the more strict the algorithm will be for clsuter_id. Default: 0.3-0.5

hashes: 7
keygroups: 4
# jaccard_similarity_threshold: ${probabilistic.jaccard_similarity_threshold} 


#---Params for clustering and similarity , use string_type cosine or jarowinkler
### Can you please add a line for each parameter below with quick guidelines on how changing value will affect algo behavior?

###---convergence_threshold ---> just controls Soft Impute for NULL values and it is not as important to modify, so we can run as default. It would only have big impact when clsuters have many records and lots of records have NULL values.

###---cluster_threshold ---> bw 0-1, so the closer to 1.0 it is, the more strict the clustering logic would be, thus reducing number of clusters taht qualify for matching, but increasing similarity between records in each cluster. Recommended values: 0.45-0.80

###---string_type ---> use string_type fuzzy or jarowinkler, Default: jarowinkler since it is a bit more compute efficient. IF you leave blank, it will use fuzzy as default which is good for if you are matching a bigger corpus of text (ex. very long address string).

###---fill_missing ---> Default: True --> applies Soft Impute to NULL values to improve clustering scoring. When set to == False, it will give you 0 similarity when values = NULL and reduce clustering score. In most cases just leave as default.

###---avg_spend_per_user ---> Default: 50 --> allows to define how much customer spends on AVG per year to send marketing campaigns to a single ID, which determines the estimated savings, shown in the final dashboard.

# convergence_threshold : ${probabilistic.convergence_threshold} 
# cluster_threshold: ${probabilistic.convergence_threshold}
string_type :
fill_missing: True
avg_spend_per_user: 100
# num_block_splits: ${probabilistic.num_block_splits}
# query_engine: ${probabilistic.query_engine}
